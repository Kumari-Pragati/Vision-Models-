{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "7d0c194752594ac5b4f46f1221f4a367": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9378e0844c6c4560b1059a8ddce98b18",
              "IPY_MODEL_33d31ee4fb0b4926a080f465c9d8c4d6",
              "IPY_MODEL_b4bd35c1c01344c496c5dc617257bfd1"
            ],
            "layout": "IPY_MODEL_3674b2f8722a4d10aa4a44ec8011a3f3"
          }
        },
        "9378e0844c6c4560b1059a8ddce98b18": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4bce9619dd6b4915b9a41a0d0a705f90",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_850c66247b054a63bcf220fc05d67bd4",
            "value": "Loadingâ€‡checkpointâ€‡shards:â€‡100%"
          }
        },
        "33d31ee4fb0b4926a080f465c9d8c4d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_15f8d11573fe4673aec87384eb77b09f",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a3746f040a474b089c0eae0d1e7fc31f",
            "value": 3
          }
        },
        "b4bd35c1c01344c496c5dc617257bfd1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ef237f7001f8471f92fa832b1a0c29fe",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_528d4664cce64b00a17bea73fdbbd1c9",
            "value": "â€‡3/3â€‡[00:55&lt;00:00,â€‡16.45s/it]"
          }
        },
        "3674b2f8722a4d10aa4a44ec8011a3f3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4bce9619dd6b4915b9a41a0d0a705f90": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "850c66247b054a63bcf220fc05d67bd4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "15f8d11573fe4673aec87384eb77b09f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a3746f040a474b089c0eae0d1e7fc31f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ef237f7001f8471f92fa832b1a0c29fe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "528d4664cce64b00a17bea73fdbbd1c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ğŸ–¼ï¸ Image Captioning using PaliGemma\n",
        "\n",
        "This notebook demonstrates how to use Google's `paligemma-3b-mix-224` vision-language model to generate captions or textual descriptions for your own uploaded image.\n",
        "\n",
        "We use Hugging Face Transformers, PyTorch, and Pillow to load the model, process the image, and generate a descriptive response.\n"
      ],
      "metadata": {
        "id": "3MJu93wrNUHD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ğŸ“¦ Install Dependencies\n",
        "\n",
        "We install the necessary libraries such as `transformers`, `torch`, and `torchvision`. These are required to load and run the PaliGemma model.\n"
      ],
      "metadata": {
        "id": "0Fy6aB4INaNL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "XRuncC6VVoh3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1d99089-45ff-431f-90f5-9dfaa676cbb8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m86.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m69.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m52.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m49.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "!pip install -q transformers accelerate torch torchvision"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "\n",
        "# Log in to Hugging Face with your token\n",
        "login(token=\"hf_XXXXXXXXXXXXXX\")  # Replace with your actual token"
      ],
      "metadata": {
        "id": "50hbvV4E1xoZ"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ğŸ“š Import Python Libraries\n",
        "\n",
        "We import essential modules including:\n",
        "- `torch` for model loading and inference\n",
        "- `transformers` for accessing PaliGemma and its processor\n",
        "- `PIL.Image` for opening and processing your image\n"
      ],
      "metadata": {
        "id": "Dj_dmyFnNins"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import torch\n",
        "from transformers import AutoProcessor, AutoModelForVision2Seq\n",
        "from PIL import Image"
      ],
      "metadata": {
        "id": "XgNcEdBJ28Km"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ğŸ”„ Load the PaliGemma Model\n",
        "\n",
        "We load the PaliGemma vision-to-language model and its processor using Hugging Face's `AutoProcessor` and `AutoModelForVision2Seq`.\n",
        "\n",
        "Make sure to use a GPU (if available) for faster processing.\n"
      ],
      "metadata": {
        "id": "_y5fOZx7N-2l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the PaliGemma model and processor\n",
        "model_id = \"google/paligemma-3b-mix-224\"\n",
        "processor = AutoProcessor.from_pretrained(model_id)\n",
        "model = AutoModelForVision2Seq.from_pretrained(model_id, torch_dtype=torch.float16, low_cpu_mem_usage=True)\n",
        "model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "7d0c194752594ac5b4f46f1221f4a367",
            "9378e0844c6c4560b1059a8ddce98b18",
            "33d31ee4fb0b4926a080f465c9d8c4d6",
            "b4bd35c1c01344c496c5dc617257bfd1",
            "3674b2f8722a4d10aa4a44ec8011a3f3",
            "4bce9619dd6b4915b9a41a0d0a705f90",
            "850c66247b054a63bcf220fc05d67bd4",
            "15f8d11573fe4673aec87384eb77b09f",
            "a3746f040a474b089c0eae0d1e7fc31f",
            "ef237f7001f8471f92fa832b1a0c29fe",
            "528d4664cce64b00a17bea73fdbbd1c9"
          ]
        },
        "id": "Beb9etSx2_Tj",
        "outputId": "f08f96c9-79cb-45d0-ab52-5dc896dc2c81"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7d0c194752594ac5b4f46f1221f4a367"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PaliGemmaForConditionalGeneration(\n",
              "  (vision_tower): SiglipVisionModel(\n",
              "    (vision_model): SiglipVisionTransformer(\n",
              "      (embeddings): SiglipVisionEmbeddings(\n",
              "        (patch_embedding): Conv2d(3, 1152, kernel_size=(14, 14), stride=(14, 14), padding=valid)\n",
              "        (position_embedding): Embedding(256, 1152)\n",
              "      )\n",
              "      (encoder): SiglipEncoder(\n",
              "        (layers): ModuleList(\n",
              "          (0-26): 27 x SiglipEncoderLayer(\n",
              "            (layer_norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
              "            (self_attn): SiglipAttention(\n",
              "              (k_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
              "              (v_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
              "              (q_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
              "              (out_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
              "            )\n",
              "            (layer_norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
              "            (mlp): SiglipMLP(\n",
              "              (activation_fn): PytorchGELUTanh()\n",
              "              (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
              "              (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (post_layernorm): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
              "    )\n",
              "  )\n",
              "  (multi_modal_projector): PaliGemmaMultiModalProjector(\n",
              "    (linear): Linear(in_features=1152, out_features=2048, bias=True)\n",
              "  )\n",
              "  (language_model): GemmaForCausalLM(\n",
              "    (model): GemmaModel(\n",
              "      (embed_tokens): Embedding(257216, 2048, padding_idx=0)\n",
              "      (layers): ModuleList(\n",
              "        (0-17): 18 x GemmaDecoderLayer(\n",
              "          (self_attn): GemmaAttention(\n",
              "            (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
              "            (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
              "            (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
              "            (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
              "          )\n",
              "          (mlp): GemmaMLP(\n",
              "            (gate_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
              "            (up_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
              "            (down_proj): Linear(in_features=16384, out_features=2048, bias=False)\n",
              "            (act_fn): PytorchGELUTanh()\n",
              "          )\n",
              "          (input_layernorm): GemmaRMSNorm((2048,), eps=1e-06)\n",
              "          (post_attention_layernorm): GemmaRMSNorm((2048,), eps=1e-06)\n",
              "        )\n",
              "      )\n",
              "      (norm): GemmaRMSNorm((2048,), eps=1e-06)\n",
              "      (rotary_emb): GemmaRotaryEmbedding()\n",
              "    )\n",
              "    (lm_head): Linear(in_features=2048, out_features=257216, bias=False)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "bez9R0NmOBdL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zPtaanQK3EGD",
        "outputId": "6a9c5707-8dce-4544-ef4d-0ea6de35b4ea"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PaliGemmaForConditionalGeneration(\n",
              "  (vision_tower): SiglipVisionModel(\n",
              "    (vision_model): SiglipVisionTransformer(\n",
              "      (embeddings): SiglipVisionEmbeddings(\n",
              "        (patch_embedding): Conv2d(3, 1152, kernel_size=(14, 14), stride=(14, 14), padding=valid)\n",
              "        (position_embedding): Embedding(256, 1152)\n",
              "      )\n",
              "      (encoder): SiglipEncoder(\n",
              "        (layers): ModuleList(\n",
              "          (0-26): 27 x SiglipEncoderLayer(\n",
              "            (layer_norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
              "            (self_attn): SiglipAttention(\n",
              "              (k_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
              "              (v_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
              "              (q_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
              "              (out_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
              "            )\n",
              "            (layer_norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
              "            (mlp): SiglipMLP(\n",
              "              (activation_fn): PytorchGELUTanh()\n",
              "              (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
              "              (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (post_layernorm): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
              "    )\n",
              "  )\n",
              "  (multi_modal_projector): PaliGemmaMultiModalProjector(\n",
              "    (linear): Linear(in_features=1152, out_features=2048, bias=True)\n",
              "  )\n",
              "  (language_model): GemmaForCausalLM(\n",
              "    (model): GemmaModel(\n",
              "      (embed_tokens): Embedding(257216, 2048, padding_idx=0)\n",
              "      (layers): ModuleList(\n",
              "        (0-17): 18 x GemmaDecoderLayer(\n",
              "          (self_attn): GemmaAttention(\n",
              "            (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
              "            (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
              "            (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
              "            (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
              "          )\n",
              "          (mlp): GemmaMLP(\n",
              "            (gate_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
              "            (up_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
              "            (down_proj): Linear(in_features=16384, out_features=2048, bias=False)\n",
              "            (act_fn): PytorchGELUTanh()\n",
              "          )\n",
              "          (input_layernorm): GemmaRMSNorm((2048,), eps=1e-06)\n",
              "          (post_attention_layernorm): GemmaRMSNorm((2048,), eps=1e-06)\n",
              "        )\n",
              "      )\n",
              "      (norm): GemmaRMSNorm((2048,), eps=1e-06)\n",
              "      (rotary_emb): GemmaRotaryEmbedding()\n",
              "    )\n",
              "    (lm_head): Linear(in_features=2048, out_features=257216, bias=False)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ğŸ–¼ï¸ Upload Your Image\n",
        "\n",
        "Replace the `image_path` with the path to your own image. Make sure the image is in `.jpg`, `.jpeg`, or `.png` format.\n",
        "\n",
        "We use `PIL.Image.open()` to load the image and convert it to RGB format.\n"
      ],
      "metadata": {
        "id": "ohEeZW-QOFx5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load your own image\n",
        "image_path = \"/content/image.png\"  # Change this to your uploaded image path\n",
        "image = Image.open(image_path).convert(\"RGB\")"
      ],
      "metadata": {
        "id": "ozFQf2oK3IK8"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ğŸ§  Define Prompt for Captioning\n",
        "\n",
        "You can provide a prompt to guide the model on what type of output you want. For example:\n",
        "- `\"Describe the image\"`\n",
        "- `\"What is happening in this image?\"`\n",
        "- `\"Write a caption for this image\"`\n"
      ],
      "metadata": {
        "id": "XZFrqxmZOlqz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define your task prompt\n",
        "prompt = \"Describe the image.\""
      ],
      "metadata": {
        "id": "mfcg9ZQi3L_y"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ğŸ”„ Preprocess Image and Prompt\n",
        "\n",
        "We process the image and the prompt using the `processor`. The result is a tensor suitable for model input.\n"
      ],
      "metadata": {
        "id": "0GbQJXNcOq12"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess input\n",
        "inputs = processor(image, text=prompt, return_tensors=\"pt\").to(device, torch.float16)\n",
        "\n",
        "# Generate output\n",
        "with torch.no_grad():\n",
        "    outputs = model.generate(**inputs, max_new_tokens=50)\n",
        "\n",
        "# Decode output\n",
        "generated_text = processor.batch_decode(outputs, skip_special_tokens=True)[0]\n",
        "\n",
        "# Print the result\n",
        "print(\"ğŸ” Model Output:\", generated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zgaBuYKw0PPx",
        "outputId": "18b72fdd-3f67-436a-dd56-086409cd96aa"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You are passing both `text` and `images` to `PaliGemmaProcessor`. The processor expects special image tokens in the text, as many tokens as there are images per each text. It is recommended to add `<image>` tokens in the very beginning of your text. For this call, we will infer how many images each text has and add special tokens.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ” Model Output: Describe the image.\n",
            "page displaying different products available with other options\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define your task prompt\n",
        "prompt = \"Can you describe the components of the image?\""
      ],
      "metadata": {
        "id": "QnGAWa664Ii2"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## âœ¨ Generate Caption\n",
        "\n",
        "We use the `generate()` method to get the model's prediction. This step performs the vision-language generation based on your image and prompt.\n",
        "\n",
        "## ğŸ“ Decode and Display Output\n",
        "\n",
        "The generated output is decoded using `processor.batch_decode()` to convert model tokens into human-readable text.\n"
      ],
      "metadata": {
        "id": "VyCHyFGfOwxV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess input\n",
        "inputs = processor(image, text=prompt, return_tensors=\"pt\").to(device, torch.float16)\n",
        "\n",
        "# Generate output\n",
        "with torch.no_grad():\n",
        "    outputs = model.generate(**inputs, max_new_tokens=50)\n",
        "\n",
        "# Decode output\n",
        "generated_text = processor.batch_decode(outputs, skip_special_tokens=True)[0]\n",
        "\n",
        "# Print the result\n",
        "print(\"ğŸ” Model Output:\", generated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ACmAsaTh49Mv",
        "outputId": "73025515-5359-4562-e4bb-382350fe4d12"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You are passing both `text` and `images` to `PaliGemmaProcessor`. The processor expects special image tokens in the text, as many tokens as there are images per each text. It is recommended to add `<image>` tokens in the very beginning of your text. For this call, we will infer how many images each text has and add special tokens.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ” Model Output: Can you describe the components of the image?\n",
            "earbuds\n"
          ]
        }
      ]
    }
  ]
}